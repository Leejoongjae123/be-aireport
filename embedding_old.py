# -*- coding: utf-8 -*-
"""
ë©€í‹°ëª¨ë‹¬ RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì—­ìˆ˜ìš”5.pdfë¥¼ ë¶„ì„í•˜ê³ 
procedure.jsonì˜ ë¶„ì•¼ë²ˆí˜¸ 1ë²ˆ ì†Œëª©ì°¨ë“¤ì— ëŒ€í•´ retrieve ìˆ˜í–‰

ë…¸íŠ¸ë¶ íŒŒì¼(10-Multi_modal_RAG-GPT-4o.ipynb)ì˜ ì ‘ê·¼ ë°©ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬ì„±
"""

import os
import io
import re
import json
import uuid
import base64
from typing import List, Dict, Any, Tuple
from pathlib import Path

from PIL import Image
from unstructured.partition.pdf import partition_pdf
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.stores import InMemoryStore
from langchain_core.documents import Document
from langchain.retrievers import MultiVectorRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_text_splitters import CharacterTextSplitter

# Tesseract ê²½ë¡œ ì„¤ì • (Windows/Linux ìë™ ê°ì§€)
import unstructured_pytesseract
import platform

# ìš´ì˜ì²´ì œì— ë”°ë¼ Tesseract ê²½ë¡œ ì„¤ì •
if platform.system() == "Windows":
    tesseract_path = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
    if os.path.exists(tesseract_path):
        unstructured_pytesseract.pytesseract.tesseract_cmd = tesseract_path
        print(f"Tesseract ê²½ë¡œ ì„¤ì • ì™„ë£Œ: {tesseract_path}")
    else:
        print(f"ê²½ê³ : Tesseractë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tesseract_path}")
else:
    # Linux/Docker í™˜ê²½ì—ì„œëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš© (apt-getìœ¼ë¡œ ì„¤ì¹˜ë¨)
    tesseract_path = "/usr/bin/tesseract"
    if os.path.exists(tesseract_path):
        print(f"Tesseract ê²½ë¡œ í™•ì¸ ì™„ë£Œ: {tesseract_path}")
    else:
        print(f"ê²½ê³ : Tesseractë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tesseract_path}")

# .env íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ì§ì ‘ ì„¤ì •í•˜ì„¸ìš”.")

# OpenAI API í‚¤ í™•ì¸
if not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = "sk-proj-..."  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½ í•„ìš”
    print("ì£¼ì˜: OpenAI API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ ì½”ë“œì—ì„œ ì§ì ‘ ì„¤ì •í•˜ì„¸ìš”.")

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================

def encode_image(image_path: str) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ì„ base64 ë¬¸ìì—´ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def looks_like_base64(sb: str) -> bool:
    """ë¬¸ìì—´ì´ base64ë¡œ ë³´ì´ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None


def is_image_data(b64data: str) -> bool:
    """base64 ë°ì´í„°ê°€ ì´ë¯¸ì§€ì¸ì§€ ì‹œì‘ ë¶€ë¶„ì„ ë³´ê³  í™•ì¸í•©ë‹ˆë‹¤."""
    image_signatures = {
        b"\xff\xd8\xff": "jpg",
        b"\x89\x50\x4e\x47\x0d\x0a\x1a\x0a": "png",
        b"\x47\x49\x46\x38": "gif",
        b"\x52\x49\x46\x46": "webp",
    }
    try:
        header = base64.b64decode(b64data)[:8]
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True
        return False
    except Exception:
        return False


def resize_base64_image(base64_string: str, size=(1300, 600)) -> str:
    """Base64 ë¬¸ìì—´ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤."""
    img_data = base64.b64decode(base64_string)
    img = Image.open(io.BytesIO(img_data))
    
    resized_img = img.resize(size, Image.LANCZOS)
    
    buffered = io.BytesIO()
    resized_img.save(buffered, format=img.format)
    
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def split_image_text_types(docs: List) -> Dict[str, List]:
    """base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    b64_images = []
    texts = []
    
    for doc in docs:
        if isinstance(doc, Document):
            doc = doc.page_content
        if looks_like_base64(doc) and is_image_data(doc):
            doc = resize_base64_image(doc, size=(1300, 600))
            b64_images.append(doc)
        else:
            texts.append(doc)
    
    return {"images": b64_images, "texts": texts}


# ============================================================================
# PDF ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================

def extract_pdf_elements(pdf_path: str, image_output_path: str) -> Tuple[List, List]:
    """
    PDF íŒŒì¼ì—ì„œ ì´ë¯¸ì§€, í…Œì´ë¸”, ê·¸ë¦¬ê³  í…ìŠ¤íŠ¸ ì¡°ê°ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        image_output_path: ì´ë¯¸ì§€(.jpg)ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        raw_pdf_elements: ì¶”ì¶œëœ ì›ë³¸ ìš”ì†Œë“¤
    """
    print(f"PDF íŒŒì¼ íŒŒí‹°ì…”ë‹ ì¤‘: {pdf_path}")
    
    return partition_pdf(
        filename=pdf_path,
        strategy="fast",  # onnxruntime ì—†ì´ ì‘ë™í•˜ëŠ” fast ì „ëµ ì‚¬ìš©
        extract_images_in_pdf=True,  # PDF ë‚´ ì´ë¯¸ì§€ ì¶”ì¶œ í™œì„±í™”
        infer_table_structure=True,  # í…Œì´ë¸” êµ¬ì¡° ì¶”ë¡  í™œì„±í™”
        chunking_strategy="by_title",  # ì œëª©ë³„ë¡œ í…ìŠ¤íŠ¸ ì¡°ê°í™”
        max_characters=4000,  # ìµœëŒ€ ë¬¸ì ìˆ˜
        new_after_n_chars=3800,  # ì´ ë¬¸ì ìˆ˜ ì´í›„ì— ìƒˆë¡œìš´ ì¡°ê° ìƒì„±
        combine_text_under_n_chars=2000,  # ì´ ë¬¸ì ìˆ˜ ì´í•˜ì˜ í…ìŠ¤íŠ¸ëŠ” ê²°í•©
        image_output_dir_path=image_output_path,  # ì´ë¯¸ì§€ ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        languages=["kor", "eng"],  # OCR ì–¸ì–´ ì„¤ì • (í•œêµ­ì–´, ì˜ì–´)
    )


def categorize_elements(raw_pdf_elements: List) -> Tuple[List[str], List[str]]:
    """
    PDFì—ì„œ ì¶”ì¶œëœ ìš”ì†Œë¥¼ í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    
    Args:
        raw_pdf_elements: unstructured.documents.elementsì˜ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        texts: í…ìŠ¤íŠ¸ ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
        tables: í…Œì´ë¸” ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
    """
    tables = []
    texts = []
    
    for element in raw_pdf_elements:
        if "unstructured.documents.elements.Table" in str(type(element)):
            tables.append(str(element))
        elif "unstructured.documents.elements.CompositeElement" in str(type(element)):
            texts.append(str(element))
    
    return texts, tables


# ============================================================================
# ìš”ì•½ ìƒì„± í•¨ìˆ˜ë“¤
# ============================================================================

def generate_text_summaries(texts: List[str], tables: List[str], summarize_texts=False) -> Tuple[List[str], List[str]]:
    """
    í…ìŠ¤íŠ¸ ìš”ì†Œ ìš”ì•½
    
    Args:
        texts: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        tables: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        summarize_texts: í…ìŠ¤íŠ¸ ìš”ì•½ ì—¬ë¶€ë¥¼ ê²°ì •. True/False
        
    Returns:
        text_summaries: í…ìŠ¤íŠ¸ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        table_summaries: í…Œì´ë¸” ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    """
    print("\ní…ìŠ¤íŠ¸ ë° í…Œì´ë¸” ìš”ì•½ ìƒì„± ì¤‘...")
    
    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt_text = """You are an assistant tasked with summarizing tables and text for retrieval. \
    These summaries will be embedded and used to retrieve the raw text or table elements. \
    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} """
    prompt = ChatPromptTemplate.from_template(prompt_text)
    
    # í…ìŠ¤íŠ¸ ìš”ì•½ ì²´ì¸
    model = ChatOpenAI(temperature=0, model="gpt-4")
    summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()
    
    # ìš”ì•½ì„ ìœ„í•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    text_summaries = []
    table_summaries = []
    
    # ì œê³µëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ìš”ì•½ì´ ìš”ì²­ë˜ì—ˆì„ ê²½ìš° ì ìš©
    if texts and summarize_texts:
        print(f"  í…ìŠ¤íŠ¸ {len(texts)}ê°œ ìš”ì•½ ì¤‘...")
        text_summaries = summarize_chain.batch(texts, {"max_concurrency": 5})
    elif texts:
        text_summaries = texts
    
    # ì œê³µëœ í…Œì´ë¸”ì— ì ìš©
    if tables:
        print(f"  í…Œì´ë¸” {len(tables)}ê°œ ìš”ì•½ ì¤‘...")
        table_summaries = summarize_chain.batch(tables, {"max_concurrency": 5})
    
    print(f"ìš”ì•½ ì™„ë£Œ - í…ìŠ¤íŠ¸: {len(text_summaries)}, í…Œì´ë¸”: {len(table_summaries)}")
    
    return text_summaries, table_summaries


def image_summarize(img_base64: str, prompt: str) -> str:
    """ì´ë¯¸ì§€ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    chat = ChatOpenAI(model="gpt-4o", max_tokens=2048)
    
    msg = chat.invoke(
        [
            HumanMessage(
                content=[
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"},
                    },
                ]
            )
        ]
    )
    return msg.content


def generate_img_summaries(path: str) -> Tuple[List[str], List[str]]:
    """
    ì´ë¯¸ì§€ì— ëŒ€í•œ ìš”ì•½ê³¼ base64 ì¸ì½”ë”©ëœ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        path: Unstructuredì— ì˜í•´ ì¶”ì¶œëœ .jpg íŒŒì¼ ëª©ë¡ì˜ ê²½ë¡œ
        
    Returns:
        img_base64_list: base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        image_summaries: ì´ë¯¸ì§€ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    """
    print("\nì´ë¯¸ì§€ ìš”ì•½ ìƒì„± ì¤‘...")
    
    # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    img_base64_list = []
    
    # ì´ë¯¸ì§€ ìš”ì•½ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    image_summaries = []
    
    # ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not os.path.exists(path):
        print(f"ê²½ê³ : ì´ë¯¸ì§€ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
        return img_base64_list, image_summaries
    
    # ê²½ë¡œ ë‚´ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    try:
        files = os.listdir(path)
    except Exception as e:
        print(f"ê²½ê³ : ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}, ì˜¤ë¥˜: {e}")
        return img_base64_list, image_summaries
    
    # jpg íŒŒì¼ í•„í„°ë§
    jpg_files = [f for f in files if f.endswith(".jpg")]
    
    if not jpg_files:
        print(f"ê²½ê³ : {path}ì— .jpg íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return img_base64_list, image_summaries
    
    # ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    prompt = """You are an assistant tasked with summarizing images for retrieval. \
    These summaries will be embedded and used to retrieve the raw image. \
    Give a concise summary of the image that is well optimized for retrieval."""
    
    # ì´ë¯¸ì§€ì— ì ìš©
    for img_file in sorted(jpg_files):
        img_path = os.path.join(path, img_file)
        base64_image = encode_image(img_path)
        img_base64_list.append(base64_image)
        image_summaries.append(image_summarize(base64_image, prompt))
    
    print(f"ì´ë¯¸ì§€ ìš”ì•½ ì™„ë£Œ - {len(image_summaries)}ê°œ")
    
    return img_base64_list, image_summaries


# ============================================================================
# ë©€í‹° ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±
# ============================================================================

def create_multi_vector_retriever(
    vectorstore, 
    text_summaries: List[str], 
    texts: List[str], 
    table_summaries: List[str], 
    tables: List[str], 
    image_summaries: List[str], 
    images: List[str]
) -> MultiVectorRetriever:
    """
    ìš”ì•½ì„ ìƒ‰ì¸í™”í•˜ì§€ë§Œ ì›ë³¸ ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        vectorstore: ë²¡í„° ìŠ¤í† ì–´
        text_summaries: í…ìŠ¤íŠ¸ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        table_summaries: í…Œì´ë¸” ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        tables: ì›ë³¸ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸
        image_summaries: ì´ë¯¸ì§€ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        images: ì›ë³¸ ì´ë¯¸ì§€(base64) ë¦¬ìŠ¤íŠ¸
        
    Returns:
        retriever: MultiVectorRetriever ê°ì²´
    """
    print("\në©€í‹° ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„± ì¤‘...")
    
    # ì €ì¥ ê³„ì¸µ ì´ˆê¸°í™”
    store = InMemoryStore()
    id_key = "doc_id"
    
    # ë©€í‹° ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key=id_key,
    )
    
    # ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì™€ ë¬¸ì„œ ì €ì¥ì†Œì— ì¶”ê°€í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    def add_documents(retriever, doc_summaries, doc_contents):
        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
        summary_docs = [
            Document(page_content=s, metadata={id_key: doc_ids[i]})
            for i, s in enumerate(doc_summaries)
        ]
        retriever.vectorstore.add_documents(summary_docs)
        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))
    
    # í…ìŠ¤íŠ¸, í…Œì´ë¸”, ì´ë¯¸ì§€ ì¶”ê°€
    if text_summaries:
        print(f"  í…ìŠ¤íŠ¸ {len(text_summaries)}ê°œ ì¶”ê°€ ì¤‘...")
        add_documents(retriever, text_summaries, texts)
    
    if table_summaries:
        print(f"  í…Œì´ë¸” {len(table_summaries)}ê°œ ì¶”ê°€ ì¤‘...")
        add_documents(retriever, table_summaries, tables)
    
    if image_summaries:
        print(f"  ì´ë¯¸ì§€ {len(image_summaries)}ê°œ ì¶”ê°€ ì¤‘...")
        add_documents(retriever, image_summaries, images)
    
    print("ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ")
    return retriever


# ============================================================================
# RAG ì²´ì¸ êµ¬ì„±
# ============================================================================

def img_prompt_func(data_dict: Dict) -> List[HumanMessage]:
    """
    ì»¨í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ê³  ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        data_dict: questionê³¼ contextë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        messages: HumanMessage ë¦¬ìŠ¤íŠ¸
    """
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    messages = []
    
    # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ì— ì¶”ê°€
    if data_dict["context"]["images"]:
        for image in data_dict["context"]["images"]:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image}"},
            }
            messages.append(image_message)
    
    # ë¶„ì„ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¶”ê°€
    text_message = {
        "type": "text",
        "text": (
            "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë° ê¸°ìˆ  ë¶„ì•¼ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n"
            "í…ìŠ¤íŠ¸, í…Œì´ë¸”, ì´ë¯¸ì§€(ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ ë“±)ê°€ í˜¼í•©ëœ ì •ë³´ë¥¼ ì œê³µë°›ì„ ê²ƒì…ë‹ˆë‹¤.\n"
            "ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: {data_dict['question']}\n\n"
            "ì œê³µëœ í…ìŠ¤íŠ¸ ë° í…Œì´ë¸”:\n"
            f"{formatted_texts}"
        ),
    }
    messages.append(text_message)
    return [HumanMessage(content=messages)]


def multi_modal_rag_chain(retriever: MultiVectorRetriever):
    """
    ë©€í‹°ëª¨ë‹¬ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        retriever: MultiVectorRetriever ê°ì²´
        
    Returns:
        chain: RAG ì²´ì¸
    """
    # ë©€í‹°ëª¨ë‹¬ LLM
    model = ChatOpenAI(temperature=0, model="gpt-4o", max_tokens=2048)
    
    # RAG íŒŒì´í”„ë¼ì¸
    chain = (
        {
            "context": retriever | RunnableLambda(split_image_text_types),
            "question": RunnablePassthrough(),
        }
        | RunnableLambda(img_prompt_func)
        | model
        | StrOutputParser()
    )
    
    return chain


# ============================================================================
# procedure.json ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================================

def load_procedure_json(file_path: str) -> Dict:
    """procedure.json íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_all_subsections(procedure_data: Dict) -> List[Dict]:
    """
    procedure.jsonì˜ ëª¨ë“  subsectionsë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        procedure_data: procedure.json ë°ì´í„°
        
    Returns:
        subsections: ëª¨ë“  subsection ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    subsections = []
    
    for section in procedure_data.get('sections', []):
        section_id = section.get('id')
        section_name = section.get('name')
        
        for subsection in section.get('subsections', []):
            if subsection.get('enabled', True):
                subsections.append({
                    'id': subsection.get('id'),
                    'name': subsection.get('name'),
                    'section_id': section_id,
                    'section_name': section_name,
                    'order': subsection.get('order'),
                    'maxChar': subsection.get('maxChar'),
                    'minChar': subsection.get('minChar')
                })
    
    return subsections


def retrieve_for_subsections(
    retriever: MultiVectorRetriever, 
    subsections: List[Dict], 
    output_dir: str = "output",
    top_k: int = 3
) -> Dict:
    """
    ê° subsectionì— ëŒ€í•´ retrievalì„ ìˆ˜í–‰í•˜ê³  ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        retriever: MultiVectorRetriever ê°ì²´
        subsections: subsection ì •ë³´ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        top_k: ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 3)
        
    Returns:
        summary: ì „ì²´ ì²˜ë¦¬ ìš”ì•½
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nì´ {len(subsections)}ê°œì˜ subsectionì— ëŒ€í•´ retrieval ìˆ˜í–‰ ì¤‘...\n")
    
    summary = {
        "total_subsections": len(subsections),
        "processed": 0,
        "output_directory": output_dir,
        "subsections": []
    }
    
    for idx, subsection in enumerate(subsections, 1):
        subsection_id = subsection['id']
        subsection_name = subsection['name']
        
        print(f"[{idx}/{len(subsections)}] {subsection_id}: {subsection_name}")
        
        # Retrieval ìˆ˜í–‰
        docs = retriever.invoke(subsection_name, limit=top_k)
        
        # Context ì¶”ì¶œ (ìµœëŒ€ 3ê°œ)
        contexts = []
        for i, doc in enumerate(docs[:top_k], 1):
            # ë¬¸ì„œ íƒ€ì… í™•ì¸
            if isinstance(doc, str):
                # base64 ì´ë¯¸ì§€ì¸ì§€ í™•ì¸
                if looks_like_base64(doc) and is_image_data(doc):
                    doc_type = "image"
                    # ì´ë¯¸ì§€ëŠ” ì „ì²´ base64 ì €ì¥
                    content = doc
                else:
                    doc_type = "text"
                    content = doc
            else:
                doc_type = "text"
                content = str(doc)
            
            contexts.append({
                "rank": i,
                "type": doc_type,
                "content": content
            })
        
        # ê° subsectionë³„ë¡œ JSON íŒŒì¼ ì €ì¥
        result_data = {
            "subsection_id": subsection_id,
            "subsection_name": subsection_name,
            "section_id": subsection['section_id'],
            "section_name": subsection['section_name'],
            "order": subsection['order'],
            "maxChar": subsection['maxChar'],
            "minChar": subsection['minChar'],
            "query": subsection_name,
            "retrieved_count": len(contexts),
            "contexts": contexts
        }
        
        # íŒŒì¼ëª…: subsection_id.json (ì˜ˆ: 1-1.json, 1-2.json)
        output_file = os.path.join(output_dir, f"{subsection_id}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"  â†’ {len(contexts)}ê°œ context ì¶”ì¶œ ì™„ë£Œ")
        print(f"  â†’ ì €ì¥: {output_file}\n")
        
        # ìš”ì•½ ì •ë³´ ì¶”ê°€
        summary['subsections'].append({
            "id": subsection_id,
            "name": subsection_name,
            "contexts_count": len(contexts),
            "output_file": output_file
        })
        summary['processed'] += 1
    
    return summary


def save_summary(summary: Dict, output_file: str):
    """ì „ì²´ ì²˜ë¦¬ ìš”ì•½ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"\nì „ì²´ ìš”ì•½ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def process_single_folder_by_name(folder_name: str, procedure_file: str = "procedure.json") -> Dict[str, Any]:
    """
    í´ë”ëª…ìœ¼ë¡œ ë‹¨ì¼ í´ë”ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ì™¸ë¶€ í˜¸ì¶œìš©)
    
    Args:
        folder_name: ì²˜ë¦¬í•  í´ë”ëª… (data í´ë” ë‚´ì˜ í•˜ìœ„ í´ë”ëª…)
        procedure_file: procedure.json íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: "procedure.json")
    
    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\nğŸ” process_single_folder_by_name í˜¸ì¶œë¨")
    print(f"   folder_name: {folder_name}")
    print(f"   procedure_file: {procedure_file}")
    
    current_dir = Path.cwd()
    data_dir = current_dir / "data"
    folder_path = data_dir / folder_name
    
    print(f"   current_dir: {current_dir}")
    print(f"   data_dir: {data_dir}")
    print(f"   folder_path: {folder_path}")
    print(f"   folder_path.exists(): {folder_path.exists()}")
    
    if not folder_path.exists():
        error_msg = f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}"
        print(f"âŒ {error_msg}")
        return {
            "folder": folder_name,
            "success": False,
            "error": error_msg
        }
    
    print(f"âœ… í´ë” ì¡´ì¬ í™•ì¸ ì™„ë£Œ, process_single_folder í˜¸ì¶œ...")
    return process_single_folder(folder_path, procedure_file)


def process_single_folder(folder_path: Path, procedure_file: str) -> Dict[str, Any]:
    """ë‹¨ì¼ í´ë”ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    folder_name = folder_path.name
    print(f"\n{'='*60}")
    print(f"ğŸ“ ì²˜ë¦¬ ì¤‘: {folder_name}")
    print(f"{'='*60}")
    
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì €ì¥
    original_cwd = os.getcwd()
    
    try:
        # í´ë”ë¡œ ì´ë™
        os.chdir(folder_path)
        
        # figuresì™€ output í´ë” ìƒì„± (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
        figures_dir = Path("figures")
        output_dir = Path("output")
        
        figures_dir.mkdir(exist_ok=True)
        output_dir.mkdir(exist_ok=True)
        
        # PDF íŒŒì¼ ì°¾ê¸° (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ)
        pdf_files = list(Path(".").glob("*.pdf"))
        if not pdf_files:
            return {
                "folder": folder_name,
                "success": False,
                "error": "PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
        
        pdf_file = pdf_files[0].name
        print(f"ğŸ“„ PDF íŒŒì¼: {pdf_file}")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ëª¨ë‘ ìƒëŒ€ ê²½ë¡œ)
        pdf_full_path = pdf_file  # PDF íŒŒì¼ (í˜„ì¬ ë””ë ‰í† ë¦¬)
        image_output_path = "figures"  # ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê²½ë¡œ
        summary_file = output_dir / "_summary.json"
        
        # 1. PDF ìš”ì†Œ ì¶”ì¶œ
        print("\n[1/10] PDF ìš”ì†Œ ì¶”ì¶œ ì¤‘...")
        raw_pdf_elements = extract_pdf_elements(pdf_full_path, image_output_path)
        
        # 2. í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ì¶”ì¶œ
        print("\n[2/10] í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ë¶„ë¥˜ ì¤‘...")
        texts, tables = categorize_elements(raw_pdf_elements)
        print(f"ì¶”ì¶œ ì™„ë£Œ - í…ìŠ¤íŠ¸: {len(texts)}ê°œ, í…Œì´ë¸”: {len(tables)}ê°œ")
        
        # 3. í…ìŠ¤íŠ¸ì— ëŒ€í•´ íŠ¹ì • í† í° í¬ê¸° ì ìš©
        print("\n[3/10] í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=4000, 
            chunk_overlap=0
        )
        joined_texts = " ".join(texts)
        texts_4k_token = text_splitter.split_text(joined_texts)
        print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ - {len(texts_4k_token)}ê°œ ì²­í¬")
        
        # 4. í…ìŠ¤íŠ¸ ë° í…Œì´ë¸” ìš”ì•½ ìƒì„±
        print("\n[4/10] í…ìŠ¤íŠ¸ ë° í…Œì´ë¸” ìš”ì•½ ìƒì„± ì¤‘...")
        text_summaries, table_summaries = generate_text_summaries(
            texts_4k_token, 
            tables, 
            summarize_texts=True
        )
        
        # 5. ì´ë¯¸ì§€ ìš”ì•½ ìƒì„±
        print("\n[5/10] ì´ë¯¸ì§€ ìš”ì•½ ìƒì„± ì¤‘...")
        img_base64_list, image_summaries = generate_img_summaries(image_output_path)
        
        # 6. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print("\n[6/10] ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        # Chroma collection ì´ë¦„ì€ ì˜ë¬¸, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´, í•˜ì´í”ˆ, ì ë§Œ í—ˆìš©
        # í•œê¸€ í´ë”ëª…ì„ ì•ˆì „í•œ ì´ë¦„ìœ¼ë¡œ ë³€í™˜ (UUID ì‚¬ìš©)
        import hashlib
        safe_collection_name = f"rag_{hashlib.md5(folder_name.encode()).hexdigest()[:16]}"
        vectorstore = Chroma(
            collection_name=safe_collection_name, 
            embedding_function=OpenAIEmbeddings()
        )
        
        # 7. ë©€í‹° ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±
        print("\n[7/10] ë©€í‹° ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„± ì¤‘...")
        retriever = create_multi_vector_retriever(
            vectorstore,
            text_summaries,
            texts_4k_token,
            table_summaries,
            tables,
            image_summaries,
            img_base64_list,
        )
        
        # 8. RAG ì²´ì¸ ìƒì„±
        print("\n[8/10] RAG ì²´ì¸ ìƒì„± ì¤‘...")
        chain = multi_modal_rag_chain(retriever)
        print("ë©€í‹°ëª¨ë‹¬ RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
        
        # 9. procedure.jsonì—ì„œ ëª¨ë“  subsection ì¶”ì¶œ
        print("\n[9/10] procedure.json ë¡œë“œ ë° subsection ì¶”ì¶œ ì¤‘...")
        # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ procedure.json ì‚¬ìš©
        root_procedure_file = Path(original_cwd) / procedure_file
        procedure_data = load_procedure_json(str(root_procedure_file))
        subsections = extract_all_subsections(procedure_data)
        print(f"ì´ {len(subsections)}ê°œì˜ subsection ì¶”ì¶œ ì™„ë£Œ")
        
        # 10. ê° subsectionì— ëŒ€í•´ retrieval ìˆ˜í–‰ ë° ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
        print("\n[10/10] ê° subsectionë³„ retrieval ìˆ˜í–‰ ë° ì €ì¥ ì¤‘...")
        summary = retrieve_for_subsections(
            retriever, 
            subsections, 
            output_dir=str(output_dir),
            top_k=3
        )
        
        # 11. ì „ì²´ ìš”ì•½ ì €ì¥
        save_summary(summary, str(summary_file))
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"âœ… {folder_name} ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ì´ ì²˜ë¦¬ëœ subsection: {summary['processed']}ê°œ")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        
        return {
            "folder": folder_name,
            "success": True,
            "processed": summary['processed'],
            "output_dir": str(output_dir)
        }
        
    except Exception as e:
        print(f"âŒ {folder_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "folder": folder_name,
            "success": False,
            "error": str(e)
        }
    finally:
        # ì›ë˜ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ë³µê·€
        os.chdir(original_cwd)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - data í´ë” ìˆœíšŒ ë° ì¡°ê±´ë¶€ ì²˜ë¦¬"""
    print("=" * 80)
    print("ğŸ”„ data í´ë” ìˆœíšŒ ë° ë©€í‹°ëª¨ë‹¬ RAG ì²˜ë¦¬")
    print("figuresì™€ output í´ë”ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬ ìˆ˜í–‰")
    print("=" * 80)
    
    # data í´ë” ê²½ë¡œ
    data_dir = Path("data")
    if not data_dir.exists():
        print(f"âŒ data í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    # procedure.json íŒŒì¼ ê²½ë¡œ (ë£¨íŠ¸ì— ìˆë‹¤ê³  ê°€ì •)
    procedure_file = "procedure.json"
    if not os.path.exists(procedure_file):
        print(f"âŒ procedure.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {procedure_file}")
        return
    
    # data í´ë” ë‚´ì˜ ëª¨ë“  í•˜ìœ„ í´ë” ìˆœíšŒ
    folders = [f for f in data_dir.iterdir() if f.is_dir()]
    folders.sort()  # ì´ë¦„ìˆœ ì •ë ¬
    
    print(f"\nğŸ“‚ ì´ {len(folders)}ê°œì˜ í´ë”ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    processed_folders = []
    skipped_folders = []
    error_folders = []
    
    for idx, folder in enumerate(folders, 1):
        print(f"\n{'='*80}")
        print(f"ì§„í–‰ ìƒí™©: [{idx}/{len(folders)}]")
        print(f"{'='*80}")
        
        figures_dir = folder / "figures"
        output_dir = folder / "output"
        
        # figures í´ë”ë‚˜ output í´ë” ì¤‘ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
        figures_exists = figures_dir.exists()
        output_exists = output_dir.exists()
        
        if figures_exists or output_exists:
            skip_reason = []
            if figures_exists:
                skip_reason.append("figures")
            if output_exists:
                skip_reason.append("output")
            print(f"â­ï¸  ê±´ë„ˆë›°ê¸°: {folder.name} ({', '.join(skip_reason)} í´ë”ê°€ ì´ë¯¸ ì¡´ì¬)")
            skipped_folders.append(folder.name)
            continue
        
        # PDF íŒŒì¼ í™•ì¸
        pdf_files = list(folder.glob("*.pdf"))
        if not pdf_files:
            print(f"âš ï¸  ê±´ë„ˆë›°ê¸°: {folder.name} (PDF íŒŒì¼ ì—†ìŒ)")
            skipped_folders.append(folder.name)
            continue
        
        # ì²˜ë¦¬ ìˆ˜í–‰
        result = process_single_folder(folder, procedure_file)
        
        if result["success"]:
            processed_folders.append(result)
        else:
            error_folders.append(result)
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ“Š ìµœì¢… ì²˜ë¦¬ ê²°ê³¼")
    print(f"{'='*80}")
    print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬: {len(processed_folders)}ê°œ")
    print(f"â­ï¸  ê±´ë„ˆë›´ í´ë”: {len(skipped_folders)}ê°œ")
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {len(error_folders)}ê°œ")
    
    if processed_folders:
        print(f"\nâœ… ì²˜ë¦¬ëœ í´ë” ëª©ë¡:")
        for result in processed_folders:
            print(f"  - {result['folder']}: {result['processed']}ê°œ subsection ì²˜ë¦¬")
    
    if skipped_folders:
        print(f"\nâ­ï¸  ê±´ë„ˆë›´ í´ë” ëª©ë¡:")
        for folder_name in skipped_folders[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            print(f"  - {folder_name}")
        if len(skipped_folders) > 10:
            print(f"  ... ì™¸ {len(skipped_folders) - 10}ê°œ")
    
    if error_folders:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ í´ë”:")
        for result in error_folders:
            print(f"  - {result['folder']}: {result['error']}")
    
    print(f"\n{'='*80}")


if __name__ == "__main__":
    main()

